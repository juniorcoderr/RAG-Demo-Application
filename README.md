# â­ **ğŸ“˜ From-Scratch RAG Pipeline using ChromaDB + Groq (Llama 3.3-70B)**

A fully custom **Retrieval-Augmented Generation (RAG)** system built **without LangChain**, designed to show how RAG works internally â€” from document loading â†’ chunking â†’ embeddings â†’ vector storage â†’ semantic search â†’ LLM reasoning â†’ conversation memory.

This project is perfect for:

* Developers learning RAG from the core
* People who want *full control* without hidden abstractions
* Beginners who want a clean, easy-to-understand pipeline
* Groq users who want ultra-fast LLM inference
* Interview / demo / portfolio use

---

# ğŸš€ **What This Project Does**

This project builds a complete RAG system from scratch that can:

- âœ… Load `.txt`, `.pdf`, `.docx` documents  
- âœ… Split large documents into small chunks  
- âœ… Convert chunks into embeddings using SentenceTransformer  
- âœ… Store embeddings in ChromaDB (persistent vector database)  
- âœ… Perform semantic search for the most relevant chunks  
- âœ… Feed context into **Groq Llama-3.3-70B** for high-quality answers  
- âœ… Handle **conversation memory**  
- âœ… Convert follow-up questions into standalone queries  
- âœ… Fully simulate ChatGPT-style chat with your own documents  

---

# ğŸ”¥ **Key Features**

### **1ï¸âƒ£ Multi-format Document Loader**

* Reads **PDFs**, **Word documents**, and **plain text**
* Automatically detects file type
* Cleans and normalizes text

### **2ï¸âƒ£ Smart Text Chunking**

* Sentence-aware splitting
* Prevents cutting in the middle of ideas
* Default chunk size = 500 characters

### **3ï¸âƒ£ ChromaDB Vector Storage**

* Persistent local vector DB
* MiniLM-L6-v2 embedding model
* Fast semantic search
* Stores metadata like file names + chunk numbers

### **4ï¸âƒ£ Groq Llama-3.3-70B Integration**

* Extremely fast & accurate responses
* Prompt includes:

  * Retrieved document context
  * Chat history
  * User question

### **5ï¸âƒ£ Follow-up Question Understanding**

Example:
User: *"What is RAG?"*
User: *"Where is it used?"* â†’ converted into â†’ *"Where is RAG used?"*

### **6ï¸âƒ£ Conversation Memory System**

Stores messages with timestamps:

```
conversations = {
    session_id: [
        { "role": "user", "content": "...", "timestamp": "..." },
    ]
}
```

### **7ï¸âƒ£ Full Conversational RAG**

You get:

* Contextual retrieval
* Memory-aware answers
* Chat-like experience

---

# ğŸ— **High-Level Architecture**

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Documents (docs/)    â”‚
                â”‚  .txt / .pdf / .docx     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Text Chunking Engine  â”‚
                â”‚    (500-char chunks)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Embedding Generator     â”‚
                â”‚ (MiniLM-L6-v2)           â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   ChromaDB Vector Store  â”‚
                â”‚ (Persistent Collection)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Semantic Search Layer  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Groq LLM (70B)         â”‚
                â”‚   + Context + Memory     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Final Answer         â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“‚ **Project Structure**

```
ğŸ“¦ RAG-from-scratch
 â”£ ğŸ“‚ docs/               # Your input documents
 â”£ ğŸ“‚ chroma_db/          # Vector DB (auto-created)
 â”£ ğŸ“œ main.py             # Full RAG pipeline
 â”£ ğŸ“œ requirements.txt
 â”£ ğŸ“œ .env                # GROQ_API_KEY=your_key_here
```

---

# ğŸ§  **How the Pipeline Works (Simple Explanation)**

### **1. Read Documents**

* PDF â†’ extract text per page
* DOCX â†’ read paragraphs
* TXT â†’ direct read

### **2. Chunk Documents**

Break text into clean pieces â†’ easier for LLM to understand.

### **3. Embed + Index into ChromaDB**

Every chunk gets:

* A unique ID
* Embedding vector
* Metadata (file + chunk number)

### **4. Semantic Search**

User question â†’ embedding â†’ find top similar chunks.

### **5. Build RAG Prompt**

LLM receives:

```
Retrieved Context
Conversation History
User Question
```

### **6. Groq Llama 3.3-70B Generates Final Answer**
AI generates the final answer.

### **7. Conversation Saved**

Used for follow-up questions.

---

# ğŸ–¥ **Technologies Used**

* **Python**
* **ChromaDB** - Vector Store
* **Sentence Transformers (MiniLM-L6-v2)** - Embeddings
* **Groq API** - Llama 3.3-70B
* **PyPDF2** - PDF reading
* **python-docx** - DOCX reading
* **dotenv** - Environment variables

---

# ğŸ“Œ Example Capabilities

### Ask questions like:

```
"What is RAG?"
"Explain chunking."
"Where is retrieval used?"
"Give real-world examples."
```

### And the system answers from YOUR documents:

```
"According to RAG_(Retrieval-Augmented_Generation).pdf, RAG is..."
```

---

# âœ… **Why This Project Is Special**

Unlike LangChain/LlamaIndex, this project gives:

âœ” Full visibility
âœ” Full control
âœ” Zero abstraction
âœ” Better debugging
âœ” Production-level transparency

Perfect for:

* Learning
* Interviews
* Real-world integrations
* Custom enterprise RAG designs

---

# ğŸ›  Setup Instructions

```
pip install -r requirements.txt
```

Add `.env`:

```
GROQ_API_KEY=your_key_here
```

Run the project:

```
python main.py
```

---

# â­ **Conclusion**

This repo is a fully working **end-to-end RAG system** built from scratch â€”
no frameworks, no shortcuts, complete transparency.

Perfect for anyone who wants to understand **how real RAG works internally**.
